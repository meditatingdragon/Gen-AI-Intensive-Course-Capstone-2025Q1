{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11455448,"sourceType":"datasetVersion","datasetId":7177724}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Real Estate Image Tagger\n\nAs I've previously worked in Real Estate, I wanted to explore use cases in real estate using GenAI. \n\nA simple use case I've considered is for an LLM to recognize key features in an image that would be useful for automatically labeling images for indexing so it can be searchable. I've seen too many images that are not adequately labeled. Does the listing have no carpet? I have to look at the images. Too often, listings are not easily searcheable across a number of attributes I would fine desirable. \n\nFor this project, I started with the GenAI capability of Image Understanding. I would see how far I would get with the time I had.\n\nAs of before the submission deadline, the following GenAI capabilities are covered in this project:\n* Structured output/JSON mode/controlled generation\n* Few-shot prompting\n* Image understanding\n* Embeddings\n* Retrieval augmented generation (RAG)\n* Vector search/vector store/vector database\n\n","metadata":{}},{"cell_type":"markdown","source":"## Prerequisites\n\nInclude retry policy and be able to use the Google API Key.","metadata":{}},{"cell_type":"code","source":"# Uninstall packages from Kaggle base image that are not needed.\n!pip uninstall -qqy jupyterlab kfp\n# Install the google-genai SDK for this codelab.\n!pip install -qU 'google-genai==1.7.0' 'chromadb==0.6.3'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import Markdown, HTML, display\n\ngenai.__version__","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# API KEY\n\nfrom kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)\n\n# Define a retry policy. The model might make multiple consecutive calls automatically\n# for a complex query, this ensures the client retries if it hits quota limits.\nfrom google.api_core import retry\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\nif not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n  genai.models.Models.generate_content = retry.Retry(\n      predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset\n\nI created a small dataset using images from Unsplash. These are images related to real estate. I added it as an input into the notebook. ","metadata":{}},{"cell_type":"code","source":"def upload_to_google(path):\n    image = client.files.upload(file=path)\n    return image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Upload to Google\n\nIn order to send it Gemini, I must save the file to Google. Below, I define some helper methods to upload the files to Google so it can be passed to Gemini for interpretation.","metadata":{}},{"cell_type":"code","source":"# delete google files, resets images stored in Google to avoid duplicates\nprint('List all images stored in Google via File API:')\nfor f in client.files.list():\n    print(' ', f.name)\n    client.files.delete(name=f.name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nimage_map = {}\n\n# list all files in the dataset and upload to google\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        path = os.path.join(dirname, filename)\n        image = upload_to_google(path)\n        image_map[image.name] = path\n        print(path)\n\nprint(image_map)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# list files\nprint('List all images stored in Google via File API:')\nlast_file = \"\"\nfor f in client.files.list():\n    last_file = f.name\n    print(' ', f.name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# captions the image as a \"hello world\" test that it works\n# this is code directly from the documentation\ndef process_image(image_path):\n    image = client.files.get(name=image_path)\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=[image, \"Caption this image.\"])\n\n    print(response.text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_image(last_file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prompt Engineering Improvements\n\nI can see that it's working now, so I can improve the prompt to get a better description of the image.\n\nIn this section, I include a few more GenAI capabiltiies: Few-shot prompting, and Structured output/JSON mode/controlled generation.","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"\nExamine the image provided. This image is part of a real estate listing.\n\nTo enhance the listing, important details need to be identified in the image to include in the listing. \nThese details should be included in the 'caption' attribute in the sample json below. It should contain at least 150 words.\n\nThe image should also be evaluated for the presence of a pool. \n\nIf a pool is present, the json output should include a boolean value indicating a pool is present.\n\nThe output should be in JSON.\n\nEXAMPLE:\nThe image of a kitchen contains an island. A tag would be \"kitchen island\".\nJSON Response:\n```\n{\n\"caption\": \"A bright and spacious kitchen with white cabinets, a blue island, and dark countertops.\",\n\"tags\": [\"kitchen island\"],\n\"has_pool\": false\n}\n\nEXAMPLE:\nThe image of a house contains a pool. A tag would be \"outdoor pool\".\nJSON Response:\n```\n{\n\"caption\": \"A picturesque two-story house with a red exterior stands nestled between lush greenery and neighboring homes under a clear blue sky.\",\n\"tags\": [\"kitchen island\"],\n\"has_pool\": true\n}\n\"\"\"\nimport typing_extensions as typing\nimport json\n\nclass ImageTags(typing.TypedDict):\n    caption: str\n    tags: list[str]\n    has_pool: bool\n\ndef process_image_tags(image_path):\n    image = client.files.get(name=image_path)\n    response = client.models.generate_content(\n            model=\"gemini-2.0-flash\",\n            config=types.GenerateContentConfig(\n            temperature=0.5,\n            response_mime_type=\"application/json\",\n            response_schema=ImageTags,\n            ),\n            contents=[image, prompt])\n\n    return json.loads(response.text) | { \"path\": image_map[image_path] }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_image_tags(last_file)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Embeddings and Vector Databases\n\nOnce the tags are identified, the information can be added to a database for querying. As the next step of this project, I'll test out Embeddings and RAGs.","metadata":{}},{"cell_type":"code","source":"for m in client.models.list():\n    if \"embedContent\" in m.supported_actions:\n        print(m.name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from chromadb import Documents, EmbeddingFunction, Embeddings\nfrom google.api_core import retry\n\nfrom google.genai import types\n\n\n# Define a helper to retry when per-minute quota is reached.\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n\nclass GeminiEmbeddingFunction(EmbeddingFunction):\n    # Specify whether to generate embeddings for documents, or queries\n    document_mode = True\n\n    @retry.Retry(predicate=is_retriable)\n    def __call__(self, input: Documents) -> Embeddings:\n        if self.document_mode:\n            embedding_task = \"retrieval_document\"\n        else:\n            embedding_task = \"retrieval_query\"\n\n        response = client.models.embed_content(\n            model=\"models/text-embedding-004\",\n            contents=input,\n            config=types.EmbedContentConfig(\n                task_type=embedding_task,\n            ),\n        )\n        return [e.values for e in response.embeddings]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\n\n# create a list of documents to be added to the database\ndocuments = []\n\nprint('List all images stored in Google via File API:')\nfor f in client.files.list():\n    document = process_image_tags(f.name)\n    documents.append(document)\n    print(' ', document)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# display images to verify results and limit # to avoid memory problems\nfor idx, document in enumerate(documents): \n    if idx < 3:\n        display(Image.open(document['path']))\n    print(' ', document)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import chromadb\n\nDB_NAME = \"real_estate_listings_db\"\n\nembed_fn = GeminiEmbeddingFunction()\nembed_fn.document_mode = True\n\nchroma_client = chromadb.Client()\nchroma_client.delete_collection(name=DB_NAME) # reset db\n\ndb = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn, metadata={\n        \"hnsw:space\": \"cosine\",\n    })\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for document in documents:\n    db.add(\n        documents=[document['caption']],\n        metadatas=[{\"tags\": ','.join(document['tags']), \"has_pool\": document['has_pool']}],\n        ids=[document['path']]\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"db.count()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Query Database\n\nAfter adding embeddings and documents to database, now the database can be queried.","metadata":{}},{"cell_type":"code","source":"embed_fn.document_mode = False\n\n# Search the Chroma DB using the specified query.\nquery = \"Show me a listing with a red exterior.\"\n\nresult = db.query(query_texts=[query], n_results=1)\n\ndisplay(Image.open(result['ids'][0][0]))\n\n\n[all_passages] = result[\"documents\"]\n\nMarkdown(all_passages[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Search the Chroma DB using the specified query.\nquery = \"Show me a two-story home.\"\n\nresult = db.query(query_texts=[query])\n\ndisplay(Image.open(result['ids'][0][0]))\n\n[all_passages] = result[\"documents\"]\n\nprint(\"Total Listings returned: \", len(all_passages))\n\nMarkdown(all_passages[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## RAGs\n\nAfter querying the database, it augments the prompt with the results and generates a \"final answer\".","metadata":{}},{"cell_type":"code","source":"query_oneline = query.replace(\"\\n\", \" \")\n\n# This prompt is where you can specify any guidance on tone, or what topics the model should stick to, or avoid.\nprompt = f\"\"\"You are a helpful and informative real estate listing agent bot that answers questions using text from the listings included below. \nTell me how many listings match my request. Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. \nStrike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n\nQUESTION: {query_oneline}\n\"\"\"\n\n# Add the retrieved documents to the prompt.\nfor passage in all_passages:\n    passage_oneline = passage.replace(\"\\n\", \" \")\n    prompt += f\"Listing Description: {passage_oneline}\\nTotal Listings: {len(all_passages)} \"\n\nprint(prompt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"answer = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=prompt)\n\nMarkdown(answer.text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Next Steps\n\n","metadata":{}}]}