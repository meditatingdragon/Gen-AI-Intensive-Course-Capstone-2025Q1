{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11455448,"sourceType":"datasetVersion","datasetId":7177724}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Real Estate Listing Agent\n\nAs I've previously worked in Real Estate, I wanted to explore use cases in real estate using GenAI. \n\nA simple use case I've considered is for an LLM to recognize key features in an image that would be useful for automatically labeling images for indexing so it can be searchable. I've seen too many images that are not adequately labeled. Does the listing have no carpet? I have to look at the images. Too often, listings are not easily searcheable across a number of attributes I would fine desirable. \n\nFor this project, I started with the GenAI capability of Image Understanding. I would see how far I would get with the time I had. By the end of this project, it evolved into an example of a reply from a real estate listing agent chatbot. While there is not a full implementation of a chatbot, the foundation for it is here, so I've decided to title the project Real Estate Listing Agent.\n\nAs of before the submission deadline, the following GenAI capabilities are covered in this project:\n* Structured output/JSON mode/controlled generation\n* Few-shot prompting\n* Image understanding\n* Embeddings\n* Retrieval augmented generation (RAG)\n* Vector search/vector store/vector database\n\n","metadata":{}},{"cell_type":"markdown","source":"## Prerequisites\n\nInclude retry policy and be able to use the Google API Key.","metadata":{}},{"cell_type":"code","source":"# Uninstall packages from Kaggle base image that are not needed.\n!pip uninstall -qqy jupyterlab kfp\n# Install the google-genai SDK for this codelab.\n!pip install -qU 'google-genai==1.7.0' 'chromadb==0.6.3'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:54:09.760998Z","iopub.execute_input":"2025-04-20T04:54:09.761301Z","iopub.status.idle":"2025-04-20T04:54:16.145299Z","shell.execute_reply.started":"2025-04-20T04:54:09.761274Z","shell.execute_reply":"2025-04-20T04:54:16.144106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import Markdown, HTML, display\n\ngenai.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:54:24.837220Z","iopub.execute_input":"2025-04-20T04:54:24.837638Z","iopub.status.idle":"2025-04-20T04:54:25.701669Z","shell.execute_reply.started":"2025-04-20T04:54:24.837599Z","shell.execute_reply":"2025-04-20T04:54:25.700937Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# API KEY\n\nfrom kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)\n\n# Define a retry policy. The model might make multiple consecutive calls automatically\n# for a complex query, this ensures the client retries if it hits quota limits.\nfrom google.api_core import retry\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\nif not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n  genai.models.Models.generate_content = retry.Retry(\n      predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:54:27.285267Z","iopub.execute_input":"2025-04-20T04:54:27.285776Z","iopub.status.idle":"2025-04-20T04:54:27.577600Z","shell.execute_reply.started":"2025-04-20T04:54:27.285750Z","shell.execute_reply":"2025-04-20T04:54:27.576910Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset\n\nI created a small dataset using images from Unsplash. These are images related to real estate. I added it as an input into the notebook. ","metadata":{}},{"cell_type":"code","source":"def upload_to_google(path):\n    image = client.files.upload(file=path)\n    return image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:54:29.326130Z","iopub.execute_input":"2025-04-20T04:54:29.326714Z","iopub.status.idle":"2025-04-20T04:54:29.331673Z","shell.execute_reply.started":"2025-04-20T04:54:29.326681Z","shell.execute_reply":"2025-04-20T04:54:29.330676Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Upload to Google\n\nIn order to send it Gemini, I must save the file to Google. Below, I define some helper methods to upload the files to Google so it can be passed to Gemini for interpretation.","metadata":{}},{"cell_type":"code","source":"# delete google files, resets images stored in Google to avoid duplicates\nprint('List all images stored in Google via File API:')\nfor f in client.files.list():\n    print(' ', f.name)\n    client.files.delete(name=f.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:54:31.096419Z","iopub.execute_input":"2025-04-20T04:54:31.096854Z","iopub.status.idle":"2025-04-20T04:54:35.095691Z","shell.execute_reply.started":"2025-04-20T04:54:31.096824Z","shell.execute_reply":"2025-04-20T04:54:35.094834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# for easy mapping of image dataset for display to verify results\nimage_map = {}\n\n# list all files in the dataset and upload to google\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        path = os.path.join(dirname, filename)\n        image = upload_to_google(path)\n        image_map[image.name] = path\n\nprint(image_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:54:36.395900Z","iopub.execute_input":"2025-04-20T04:54:36.396511Z","iopub.status.idle":"2025-04-20T04:54:54.402332Z","shell.execute_reply.started":"2025-04-20T04:54:36.396464Z","shell.execute_reply":"2025-04-20T04:54:54.401414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# list files\nprint('List all images stored in Google via File API:')\nlast_file = \"\"\nfor f in client.files.list():\n    last_file = f.name\n    print(' ', f.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:54:56.850606Z","iopub.execute_input":"2025-04-20T04:54:56.850964Z","iopub.status.idle":"2025-04-20T04:54:57.100933Z","shell.execute_reply.started":"2025-04-20T04:54:56.850939Z","shell.execute_reply":"2025-04-20T04:54:57.100060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# captions the image as a \"hello world\" test that it works\n# this is code directly from the documentation\ndef process_image(image_path):\n    image = client.files.get(name=image_path)\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=[image, \"Caption this image.\"])\n\n    print(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:54:58.966988Z","iopub.execute_input":"2025-04-20T04:54:58.967405Z","iopub.status.idle":"2025-04-20T04:54:58.973380Z","shell.execute_reply.started":"2025-04-20T04:54:58.967338Z","shell.execute_reply":"2025-04-20T04:54:58.972321Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_image(last_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:55:02.613297Z","iopub.execute_input":"2025-04-20T04:55:02.614015Z","iopub.status.idle":"2025-04-20T04:55:04.904776Z","shell.execute_reply.started":"2025-04-20T04:55:02.613988Z","shell.execute_reply":"2025-04-20T04:55:04.903867Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prompt Engineering Improvements\n\nI can see that it's working now, so I can improve the prompt to get a better description of the image.\n\nIn this section, I include a few more GenAI capabiltiies: Few-shot prompting, and Structured output/JSON mode/controlled generation.","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"\nExamine the image provided. This image is part of a real estate listing.\n\nTo enhance the listing, important details need to be identified in the image to include in the listing. \nA title is needed that includes at least 3 key attributes visible in the image that would \nmake the listing marketable and appealing to a renter or a buyer.\nThese details should be included in the 'title' attribute in the sample json below. \nIt should be followed by a 'description' attribute that contains at least 150 words and describes the photo in detail and \nhighlights the key attributes and additional attributes that increase the value of the listing.\n\nThe image should also be evaluated for the presence of a pool. \n\nIf a pool is present, the json output should include a boolean value indicating a pool is present.\n\nThe output should be in JSON.\n\nEXAMPLE:\nThe image of a kitchen contains an island. A tag would be \"kitchen island\".\nJSON Response:\n```\n{\n\"title\": \"A beautiful kitchen with an island in the center and a large sink.\"\n\"description\": \"A bright and spacious kitchen with white cabinets, a blue island, and dark countertops. The white cabinets have ample space for pots, pans, silverware, tupperware, bakeware, mixing bowls and other kitchen tools. The pendant lights brighten the kitchen as does the cathedral windows in the ceiling that allow natural light into the kitchen illuminating the countertops. The stainless steel refrigerator contains ample space.\",\n\"tags\": [\"kitchen island\"],\n\"has_pool\": false\n}\n\nEXAMPLE:\nThe image of a house contains a pool. A tag would be \"outdoor pool\".\nJSON Response:\n```\n{\n\"title\": \"A picturesque two-story house with a red exterior stands nestled between lush greenery and neighboring homes under a clear blue sky.\",\n\"description\": \"This beautiful two-story home is a perfect starter home with a bright red exterior giving it a natural rustic appeal. The landscaping in the front blooms in the spring while the evergreens give it a perpertual green in the winter. The porch contains enough space for two chairs to enjoy the evening breeze and read a book with a cup of tea.\",\n\"tags\": [\"kitchen island\"],\n\"has_pool\": true\n}\n\"\"\"\nimport typing_extensions as typing\nimport json\n\nclass ImageTags(typing.TypedDict):\n    title: str\n    description: str\n    tags: list[str]\n    has_pool: bool\n\ndef process_image_tags(image_path):\n    image = client.files.get(name=image_path)\n    response = client.models.generate_content(\n            model=\"gemini-2.0-flash\",\n            config=types.GenerateContentConfig(\n            temperature=0.5,\n            response_mime_type=\"application/json\",\n            response_schema=ImageTags,\n            ),\n            contents=[image, prompt])\n\n    return json.loads(response.text) | { \"path\": image_map[image_path] }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:55:05.790548Z","iopub.execute_input":"2025-04-20T04:55:05.790914Z","iopub.status.idle":"2025-04-20T04:55:05.801574Z","shell.execute_reply.started":"2025-04-20T04:55:05.790884Z","shell.execute_reply":"2025-04-20T04:55:05.800655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_image_tags(last_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:55:09.969615Z","iopub.execute_input":"2025-04-20T04:55:09.970324Z","iopub.status.idle":"2025-04-20T04:55:13.878351Z","shell.execute_reply.started":"2025-04-20T04:55:09.970293Z","shell.execute_reply":"2025-04-20T04:55:13.877451Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Embeddings and Vector Databases\n\nOnce the tags are identified, the information can be added to a database for querying. As the next step of this project, I'll test out Embeddings and RAGs.","metadata":{}},{"cell_type":"code","source":"for m in client.models.list():\n    if \"embedContent\" in m.supported_actions:\n        print(m.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:55:14.891182Z","iopub.execute_input":"2025-04-20T04:55:14.892227Z","iopub.status.idle":"2025-04-20T04:55:14.954795Z","shell.execute_reply.started":"2025-04-20T04:55:14.892187Z","shell.execute_reply":"2025-04-20T04:55:14.953803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from chromadb import Documents, EmbeddingFunction, Embeddings\nfrom google.api_core import retry\n\nfrom google.genai import types\n\n\n# Define a helper to retry when per-minute quota is reached.\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n\nclass GeminiEmbeddingFunction(EmbeddingFunction):\n    # Specify whether to generate embeddings for documents, or queries\n    document_mode = True\n\n    @retry.Retry(predicate=is_retriable)\n    def __call__(self, input: Documents) -> Embeddings:\n        if self.document_mode:\n            embedding_task = \"retrieval_document\"\n        else:\n            embedding_task = \"retrieval_query\"\n\n        response = client.models.embed_content(\n            model=\"models/text-embedding-004\",\n            contents=input,\n            config=types.EmbedContentConfig(\n                task_type=embedding_task,\n            ),\n        )\n        return [e.values for e in response.embeddings]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:55:54.272509Z","iopub.execute_input":"2025-04-20T04:55:54.273536Z","iopub.status.idle":"2025-04-20T04:55:54.280797Z","shell.execute_reply.started":"2025-04-20T04:55:54.273500Z","shell.execute_reply":"2025-04-20T04:55:54.279588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\n\n# create a list of documents to be added to the database\ndocuments = []\n\nprint('Processing all image files with Gemini API...')\nfor f in client.files.list():\n    document = process_image_tags(f.name)\n    documents.append(document)\n    print(' ', document)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:55:57.103974Z","iopub.execute_input":"2025-04-20T04:55:57.104378Z","iopub.status.idle":"2025-04-20T04:56:34.260635Z","shell.execute_reply.started":"2025-04-20T04:55:57.104326Z","shell.execute_reply":"2025-04-20T04:56:34.259679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# display images to verify results and limit # to avoid memory problems\nfor idx, document in enumerate(documents): \n    if idx < 3:\n        display(Image.open(document['path']))\n    print(' ', document)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import chromadb\n\nDB_NAME = \"real_estate_listings_db\"\n\nembed_fn = GeminiEmbeddingFunction()\nembed_fn.document_mode = True\n\nchroma_client = chromadb.Client()\n# chroma_client.delete_collection(name=DB_NAME) # reset db\n\ndb = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:59:24.400926Z","iopub.execute_input":"2025-04-20T04:59:24.401298Z","iopub.status.idle":"2025-04-20T04:59:24.413045Z","shell.execute_reply.started":"2025-04-20T04:59:24.401271Z","shell.execute_reply":"2025-04-20T04:59:24.411921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for document in documents:\n    document_text = \"Title: \" + document['title'] + \"\\nDescription: \" + document['description']\n    print(document_text)\n    db.add(\n        documents=[document['title']],\n        metadatas=[{\"tags\": ','.join(document['tags']), \"has_pool\": document['has_pool'], \"description\": document['description']}],\n        ids=[document['path']]\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:59:26.140005Z","iopub.execute_input":"2025-04-20T04:59:26.140346Z","iopub.status.idle":"2025-04-20T04:59:27.989962Z","shell.execute_reply.started":"2025-04-20T04:59:26.140324Z","shell.execute_reply":"2025-04-20T04:59:27.989150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"db.count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:59:30.163068Z","iopub.execute_input":"2025-04-20T04:59:30.163432Z","iopub.status.idle":"2025-04-20T04:59:30.173134Z","shell.execute_reply.started":"2025-04-20T04:59:30.163402Z","shell.execute_reply":"2025-04-20T04:59:30.172150Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Query Database\n\nAfter adding embeddings and documents to database, now the database can be queried.","metadata":{}},{"cell_type":"code","source":"embed_fn.document_mode = False\n\n# Search the Chroma DB using the specified query.\nquery = \"Show me a listing with a red exterior.\"\n\nresult = db.query(query_texts=[query], n_results=1)\n\ndisplay(Image.open(result['ids'][0][0]))\n\n\n[all_passages] = result[\"documents\"]\n\nMarkdown(all_passages[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embed_fn.document_mode = False\n\n# Search the Chroma DB using the specified query.\nquery = \"Show me a two-story home.\"\n\nresult = db.query(query_texts=[query], n_results=1)\n\ndisplay(Image.open(result['ids'][0][0]))\n\n\n[all_passages] = result[\"documents\"]\n\nMarkdown(all_passages[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Search the Chroma DB using the specified query.\nquery = \"Show me listings with a pool.\"\n\nresult = db.query(query_texts=[query], n_results=3)\n\ndisplay(Image.open(result['ids'][0][0]))\n\n[all_passages] = result[\"documents\"]\n\nprint(\"Distances: \", result[\"distances\"])\n\nMarkdown(all_passages[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## RAGs\n\nAfter querying the database, it augments the prompt with the results and generates a \"final answer\".","metadata":{}},{"cell_type":"code","source":"query_oneline = query.replace(\"\\n\", \" \")\n\n# This prompt is where you can specify any guidance on tone, or what topics the model should stick to, or avoid.\nprompt = f\"\"\"You are a helpful and informative real estate listing agent bot that answers questions using information \nfrom the listings included below. \nBe sure to respond in a complete sentence, being comprehensive, including all relevant background information. \nStrike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n\nQUESTION: {query_oneline}\n\"\"\"\n\n# Add the retrieved documents to the prompt.\nfor idx, passage in enumerate(all_passages):\n    passage_oneline = passage.replace(\"\\n\", \" \")\n    prompt += f\"Listing {idx + 1}: {passage_oneline} \\n\"\n\nprint(prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T05:04:55.985466Z","iopub.execute_input":"2025-04-20T05:04:55.985869Z","iopub.status.idle":"2025-04-20T05:04:55.992303Z","shell.execute_reply.started":"2025-04-20T05:04:55.985836Z","shell.execute_reply":"2025-04-20T05:04:55.991309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"answer = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=prompt)\n\nMarkdown(answer.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T05:05:09.344475Z","iopub.execute_input":"2025-04-20T05:05:09.344849Z","iopub.status.idle":"2025-04-20T05:05:10.170131Z","shell.execute_reply.started":"2025-04-20T05:05:09.344823Z","shell.execute_reply":"2025-04-20T05:05:10.169121Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Next Steps\n\nAfter all this experimentation, there were definitely areas I would need to continue to work more on to make it useful.  As a larger project, I could see how this work could be extended to build a real estate agent chat bot that could help personalize the property renting or buying experience. \n\nThere were definitely some challenges I came across as I worked on different aspects of this project though. For example, there were some iterations of titles and descriptions for the images where the results were hallucinations of the actual image. In one particular image, it described it as a \"waterfront\" home, when there was really no evidence of the home being a \"waterfront\" home (i.e. no beaches, lakes, etc.) These hallucinations would need to be addressed through negative prompting or fine-tuning.\n\nI also think the results could be quite interesting if allowed to interpret multiple images as a collective set so it could make sense of multiple images as a single property listing. In this project, I mostly experimented with each image as an individual listing for simplicity sake. I would certainly be interested in experimenting with images as collections for a single property to see what could be accomplished with more content.","metadata":{}}]}